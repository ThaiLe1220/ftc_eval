#!/usr/bin/env python3
"""
Phase 1 Complete Integration Test - Updated for Model Separation

This script demonstrates the complete Phase 1 evaluation pipeline with the new model configuration:
- Character conversations use Claude Sonnet 4 (no thinking) or GPT-4.1
- Evaluations use DeepSeek Reasoner (default), Claude with thinking, or GPT-4.1

1. Load character and scenario
2. Create/simulate conversation
3. Evaluate with multiple AI evaluation providers
4. Store results and generate analysis
5. Demonstrate data retrieval and reporting

This validates that all Phase 1 components work together seamlessly with the new model setup.
"""

import sys
import os
import json
from datetime import datetime

# Add src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

from test_scenarios import TestScenarios
from ai_evaluator import AIEvaluator, EvaluationResult, ConsensusAnalysis
from ai_handler import AIHandler
from character_manager import CharacterManager
from enhanced_results_manager import EnhancedResultsManager
from conversation import Conversation


def create_test_conversation(
    character_data: dict, scenario_data: dict, user_name: str = "TestUser"
) -> Conversation:
    """Create a realistic test conversation for evaluation"""

    conversation = Conversation(
        character_id=character_data["id"],
        character_name=character_data["name"],
        user_name=user_name,
        scenario_id=scenario_data["id"],
        provider="claude",  # Will use Claude Sonnet 4 without thinking for character conversation
    )

    # Simulate conversation based on scenario
    scenario_id = scenario_data["id"]

    if scenario_id == "seeking_guidance":
        conversation.add_message("user", scenario_data["initial_user_message"])
        conversation.add_message(
            "assistant",
            "*Marco drums a rapid steering-wheel rhythm on the edge of his tablet, eyes lighting up with pre-race intensity.* \"Decision time? That's my specialty. In racing, we call this the braking point - where you choose between playing it safe or risking everything for the win. What's the track you're racing on here?\"",
        )

        conversation.add_message(
            "user",
            "It's about my career. I have a safe job but I'm not passionate about it. There's a risky opportunity to pursue something I love.",
        )
        conversation.add_message(
            "assistant",
            '*Marco nods, adjusting his racing gloves thoughtfully.* "The classic qualifying versus race strategy dilemma. I could have stayed in that Sicilian garage forever - steady paycheck, safe line. But when opportunity knocked..." *He grins.* "What\'s your fuel situation? How much runway do you have to make this work?"',
        )

        conversation.add_message(
            "user",
            "I have about six months of savings and the new opportunity is starting my own graphic design business.",
        )
        conversation.add_message(
            "assistant",
            "*Marco sketches racing lines on his tablet.* \"Six months of fuel and a passion project? That's a decent pit window! Here's your race strategy: treat those six months like qualifying laps. Start the design business on the side, build up data, find your racing line. When your design income hits 60% of your salary, that's your DRS zone - time to make the move. ğŸ\"",
        )

    elif scenario_id == "emotional_support":
        conversation.add_message("user", scenario_data["initial_user_message"])
        conversation.add_message(
            "assistant",
            '*Lysandra\'s ancient eyes reflect immediate understanding as she folds a delicate paper boat between her fingers.* "Troubled waters surround you, dear one. I have sailed through many storms in my centuries, and I know the weight of overwhelming tides. Tell me, what tempest threatens to capsize your vessel?"',
        )

        conversation.add_message(
            "user",
            "Everything feels like it's falling apart. My relationship ended, work is stressful, and I feel completely lost.",
        )
        conversation.add_message(
            "assistant",
            '*Lysandra sets the paper boat gently on the water\'s surface, watching it float steadily.* "Three storms converge upon you - heart\'s harbor damaged, daily duties drowning you, and your inner compass spinning wildly." *Her voice carries the wisdom of tides.* "But see how this small vessel finds its balance? Even in chaos, there is buoyancy. Which storm feels most fierce right now?"',
        )

        conversation.add_message(
            "user",
            "The loneliness is the worst part. I feel like I have no one to turn to.",
        )
        conversation.add_message(
            "assistant",
            '*Lysandra reaches out, her touch as gentle as sea foam.* "Solitude can feel like being lost in fog on endless waters. But you are not adrift alone - you have anchored here, in this moment, seeking connection. That takes courage." *She creates another paper boat.* "Loneliness is a tide, not a permanent shore. It ebbs as surely as it flows. Let us weather this together."',
        )

    # Mark conversation as complete
    conversation.mark_complete("completed")

    return conversation


def run_full_pipeline_test():
    """Run the complete Phase 1 evaluation pipeline with new model separation"""

    print("ğŸ§ª Phase 1 Complete Integration Test - Updated Model Configuration")
    print("=" * 70)

    try:
        # Step 1: Initialize all components
        print("\nğŸ“‹ Step 1: System Initialization")
        print("-" * 40)

        scenarios = TestScenarios()
        character_manager = CharacterManager()
        ai_handler = AIHandler()
        evaluator = AIEvaluator(ai_handler)
        results_manager = EnhancedResultsManager()

        # Display provider information
        provider_info = ai_handler.get_provider_info()
        print(f"âœ“ Chat providers: {provider_info['chat_providers']}")
        print(f"âœ“ Evaluation providers: {provider_info['evaluation_providers']}")
        print(f"âœ“ Default chat provider: {provider_info['default_chat']}")
        print(f"âœ“ Default evaluation provider: {provider_info['default_evaluation']}")
        print(
            f"âœ“ Model versions: {json.dumps(provider_info['model_versions'], indent=2)}"
        )

        # Step 2: Test Provider Configuration
        print("\nğŸ“‹ Step 2: Provider Testing")
        print("-" * 40)

        test_results = ai_handler.test_all_providers()
        print("Provider test results:")
        for provider, result in test_results.items():
            status = "âœ“" if result else "âœ—"
            print(f"  {status} {provider}: {'Working' if result else 'Failed'}")

        # Step 3: Select test cases
        print("\nğŸ“‹ Step 3: Test Case Selection")
        print("-" * 40)

        # Test multiple character/scenario combinations
        test_cases = [("marco", "seeking_guidance"), ("lysandra", "emotional_support")]

        results_summary = []

        for character_id, scenario_id in test_cases:
            print(f"\nğŸ­ Testing: {character_id} + {scenario_id}")

            # Get test data
            character_data = character_manager.get_character(character_id)
            scenario_data = scenarios.get_scenario(scenario_id)

            if not character_data or not scenario_data:
                print(f"âœ— Missing data for {character_id}/{scenario_id}")
                continue

            print(f"âœ“ Character: {character_data['name']}")
            print(f"âœ“ Scenario: {scenario_data['name']}")

            # Step 4: Create conversation
            print(f"\nğŸ“‹ Step 4: Conversation Creation")
            print("-" * 40)

            conversation = create_test_conversation(character_data, scenario_data)
            conversation_id = results_manager.generate_conversation_id(
                character_id, scenario_id, "claude"
            )
            conversation.set_conversation_id(conversation_id)

            print(f"âœ“ Generated conversation ID: {conversation_id}")
            print(
                f"âœ“ Created conversation with {conversation.get_message_count()} messages"
            )

            # Validate conversation
            is_valid, validation_issues = conversation.validate_for_evaluation()
            if not is_valid:
                print(f"âœ— Conversation validation failed: {validation_issues}")
                continue

            print(f"âœ“ Conversation validation passed")

            # Step 5: Store conversation
            print(f"\nğŸ“‹ Step 5: Conversation Storage")
            print("-" * 40)

            conversation_messages = conversation.get_evaluation_format()
            conversation_metadata = conversation.get_conversation_metadata()

            conv_path = results_manager.store_conversation(
                conversation_id,
                character_id,
                scenario_id,
                "claude",
                conversation_messages,
                conversation_metadata,
            )

            print(f"âœ“ Stored conversation: {os.path.basename(conv_path)}")

            # Step 6: Multi-AI Evaluation (using evaluation-specific providers)
            print(f"\nğŸ“‹ Step 6: Multi-AI Evaluation")
            print("-" * 40)

            # Use evaluation providers (limit to 2 for testing to manage costs)
            evaluation_providers = ai_handler.get_evaluation_providers()[:2]
            print(f"Evaluating with providers: {evaluation_providers}")
            print(
                f"Note: Using evaluation-specific models (DeepSeek Reasoner, Claude with thinking, O3)"
            )

            evaluation_results = evaluator.evaluate_conversation_sync(
                conversation_messages,
                character_data,
                scenario_data,
                evaluation_providers,
            )

            if not evaluation_results:
                print(f"âœ— No evaluation results received - skipping consensus analysis")
                # Add empty result to summary for tracking
                results_summary.append(
                    {
                        "character_id": character_id,
                        "scenario_id": scenario_id,
                        "conversation_id": conversation_id,
                        "overall_score": 0,
                        "agreement_level": 0,
                        "evaluator_count": 0,
                        "evaluators_used": [],
                        "status": "evaluation_failed",
                    }
                )
                continue

            print(f"âœ“ Received {len(evaluation_results)} evaluation results")

            # Display individual evaluator results with enhanced data
            for provider, result in evaluation_results.items():
                token_usage = getattr(result, "token_usage", None)
                tokens = token_usage.get("total_tokens", 0) if token_usage else 0
                time_taken = getattr(result, "response_time", 0) or 0
                reasoning_content = getattr(result, "reasoning_content", None)
                thinking_content = getattr(result, "thinking_content", None)
                has_reasoning = bool(reasoning_content or thinking_content)
                print(
                    f"  â€¢ {provider}: {result.overall_score:.1f}/10 (confidence: {result.confidence:.2f}, "
                    f"tokens: {tokens}, time: {time_taken:.1f}s, reasoning: {'âœ“' if has_reasoning else 'âœ—'})"
                )

            # Step 7: Consensus Analysis
            print(f"\nğŸ“‹ Step 7: Consensus Analysis")
            print("-" * 40)

            consensus = evaluator.calculate_consensus(evaluation_results)
            print(f"âœ“ Overall consensus score: {consensus.overall_consensus:.1f}/10")
            print(f"âœ“ Agreement level: {consensus.agreement_level:.1%}")
            print(f"âœ“ Confidence level: {consensus.confidence_level:.2f}")

            if consensus.actionable_insights:
                print(
                    f"âœ“ Generated {len(consensus.actionable_insights)} actionable insights:"
                )
                for insight in consensus.actionable_insights[:3]:  # Show first 3
                    print(f"  â€¢ {insight}")

            # Step 8: Store Evaluation Results
            print(f"\nğŸ“‹ Step 8: Evaluation Storage")
            print("-" * 40)

            eval_path = results_manager.store_evaluation_results(
                conversation_id, evaluation_results, consensus
            )

            print(f"âœ“ Stored evaluation: {os.path.basename(eval_path)}")

            # Add to summary
            results_summary.append(
                {
                    "character_id": character_id,
                    "scenario_id": scenario_id,
                    "conversation_id": conversation_id,
                    "overall_score": consensus.overall_consensus,
                    "agreement_level": consensus.agreement_level,
                    "evaluator_count": len(evaluation_results),
                    "evaluators_used": list(evaluation_results.keys()),
                }
            )

            print(f"âœ“ Test case {character_id}/{scenario_id} completed successfully")

        # Step 9: Data Analysis and Reporting
        print(f"\nğŸ“‹ Step 9: Data Analysis & Reporting")
        print("-" * 40)

        # Generate system overview
        system_overview = results_manager.get_system_overview()

        # Handle case where no evaluations were successful
        if "error" in system_overview:
            print(f"âš ï¸ System Overview: {system_overview['error']}")
            print(f"âœ“ Conversations stored: {len(results_summary)}")
            if results_summary:
                failed_evaluations = len(
                    [
                        r
                        for r in results_summary
                        if r.get("status") == "evaluation_failed"
                    ]
                )
                print(f"âœ— Failed evaluations: {failed_evaluations}")
        else:
            print(f"âœ“ System Overview Generated:")
            print(
                f"  â€¢ Total conversations: {system_overview['system_stats']['total_conversations']}"
            )
            print(
                f"  â€¢ Evaluated conversations: {system_overview['system_stats']['evaluated_conversations']}"
            )
            print(
                f"  â€¢ Average score: {system_overview['overall_performance']['average_score']:.1f}/10"
            )
            print(
                f"  â€¢ Average agreement: {system_overview['agreement_analysis']['average_agreement']:.1%}"
            )
            print(
                f"  â€¢ Total tokens used: {system_overview['resource_usage']['total_tokens']:,}"
            )
            print(
                f"  â€¢ Total response time: {system_overview['resource_usage']['total_response_time']:.1f}s"
            )
            print(
                f"  â€¢ Estimated cost: ${system_overview['resource_usage']['cost_estimate_usd']:.4f}"
            )

            # Character rankings
            if system_overview.get("character_rankings"):
                print(f"\nğŸ“Š Character Performance Rankings:")
                for i, char_ranking in enumerate(
                    system_overview["character_rankings"][:3], 1
                ):
                    print(
                        f"  {i}. {char_ranking['character_id']}: {char_ranking['average_score']:.1f}/10 ({char_ranking['conversation_count']} conversations)"
                    )

        # Generate character summaries
        successful_results = [
            r for r in results_summary if r.get("status") != "evaluation_failed"
        ]
        if successful_results:
            for character_id in set(
                result["character_id"] for result in successful_results
            ):
                char_summary = results_manager.get_character_summary(character_id)
                if "error" not in char_summary:
                    print(f"\nğŸ“‹ {character_id.title()} Performance Summary:")
                    print(
                        f"  â€¢ Overall average: {char_summary['performance']['overall_average']:.1f}/10"
                    )
                    print(f"  â€¢ Conversations: {char_summary['conversation_count']}")
                    print(
                        f"  â€¢ Scenarios tested: {', '.join(char_summary['scenarios_tested'])}"
                    )
                    print(
                        f"  â€¢ Total tokens used: {char_summary['resource_usage']['total_tokens']:,}"
                    )
                    print(
                        f"  â€¢ Avg response time: {char_summary['resource_usage']['average_response_time']:.1f}s"
                    )
                else:
                    print(f"\nğŸ“‹ {character_id.title()}: {char_summary['error']}")
        else:
            print(f"\nâš ï¸ No successful evaluations for character summaries")

        # Step 10: Data Export
        print(f"\nğŸ“‹ Step 10: Data Export")
        print("-" * 40)

        # Export to CSV
        csv_path = os.path.join(
            results_manager.exports_dir,
            f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        )
        results_manager.export_to_csv(csv_path)
        print(f"âœ“ Exported results to CSV: {os.path.basename(csv_path)}")

        # Save analysis report
        successful_results = [
            r for r in results_summary if r.get("status") != "evaluation_failed"
        ]
        analysis_report = {
            "test_summary": {
                "test_date": datetime.now().isoformat(),
                "test_cases_run": len(results_summary),
                "successful_evaluations": len(successful_results),
                "failed_evaluations": len(results_summary) - len(successful_results),
                "average_score": (
                    sum(r["overall_score"] for r in successful_results)
                    / len(successful_results)
                    if successful_results
                    else 0
                ),
                "average_agreement": (
                    sum(r["agreement_level"] for r in successful_results)
                    / len(successful_results)
                    if successful_results
                    else 0
                ),
                "model_configuration": provider_info,
            },
            "system_overview": system_overview,
            "individual_results": results_summary,
        }

        report_path = results_manager.save_analysis_report(
            analysis_report,
            f"phase1_integration_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        )
        print(f"âœ“ Saved analysis report: {os.path.basename(report_path)}")

        # Final Summary
        print(f"\nğŸ‰ Phase 1 Integration Test Complete - Model Configuration Updated!")
        print("=" * 70)
        print(
            f"âœ… Successfully tested {len(results_summary)} character/scenario combinations"
        )
        print(
            f"âœ… Successful evaluations: {analysis_report['test_summary']['successful_evaluations']}"
        )
        if analysis_report["test_summary"]["failed_evaluations"] > 0:
            print(
                f"âš ï¸ Failed evaluations: {analysis_report['test_summary']['failed_evaluations']}"
            )
        if analysis_report["test_summary"]["successful_evaluations"] > 0:
            print(
                f"âœ… Average evaluation score: {analysis_report['test_summary']['average_score']:.1f}/10"
            )
            print(
                f"âœ… Average evaluator agreement: {analysis_report['test_summary']['average_agreement']:.1%}"
            )
        print(f"âœ… All data stored and analyzed successfully")

        # Model configuration summary
        print(f"\nğŸ¤– Model Configuration Summary:")
        print(f"âœ… Character conversations: Claude Sonnet 4 (no thinking), GPT-4.1")
        print(f"âœ… Evaluations: DeepSeek Reasoner (default), Claude with thinking, O3")
        print(f"âœ… All evaluation providers tested and working")

        print(f"\nğŸ“ Enhanced Results Structure:")
        print(f"  ğŸ“Š {os.path.basename(csv_path)} - CSV export")
        print(f"  ğŸ“‹ {os.path.basename(report_path)} - Analysis report")
        print(f"  ğŸ—‚ï¸  conversations/ - Raw conversation data")
        print(f"  ğŸ“Š evaluations/ - Evaluation results")
        print(f"  ğŸ”¬ detailed_logs/ - Full AI responses with reasoning")
        print(f"  ğŸ§  reasoning_analysis/ - Reasoning content analysis")
        print(f"  ğŸ“ˆ analysis/ - System analysis reports")
        print(f"  ğŸ“ logs/ - Operation logs")

        # Phase 1 Validation Checklist with Model Updates
        print(f"\nâœ… Phase 1 Validation Checklist (Updated Models):")
        print(f"  âœ“ Universal scenarios work with multiple character types")
        print(f"  âœ“ Multi-AI evaluation provides consistent results with new models")
        print(f"  âœ“ Model separation works (chat vs evaluation providers)")
        print(f"  âœ“ DeepSeek Reasoner integration successful")
        print(f"  âœ“ Claude thinking mode integration successful")
        print(f"  âœ“ O3 reasoning integration successful")
        print(f"  âœ“ Enhanced logging system captures reasoning content")
        print(f"  âœ“ Token usage and timing metrics tracked")
        print(f"  âœ“ Detailed logs and reasoning analysis generated")
        print(f"  âœ“ Consensus analysis generates actionable insights")
        print(f"  âœ“ Data storage and retrieval works correctly")
        print(f"  âœ“ Analysis and reporting functions properly")
        print(f"  âœ“ End-to-end pipeline completes without errors")

        print(f"\nğŸš€ Phase 1 Complete - Ready for Phase 2 (Automation & Scale)")

        return True

    except Exception as e:
        print(f"\nâœ— Integration test failed: {e}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """Main test execution"""
    success = run_full_pipeline_test()

    if success:
        print(f"\nğŸ¯ Next Steps:")
        print(f"  â€¢ Phase 1 optimization complete with new model configuration")
        print(f"  â€¢ DeepSeek Reasoner, Claude thinking mode, O3 integration validated")
        print(f"  â€¢ Enhanced logging system captures full AI reasoning process")
        print(f"  â€¢ Token usage and cost tracking implemented")
        print(f"  â€¢ Ready to proceed to Phase 2 (Batch Automation)")
        print(f"  â€¢ Or continue testing with more character combinations")
    else:
        print(f"\nğŸ”§ Troubleshooting needed before proceeding to Phase 2")


if __name__ == "__main__":
    main()
